name: Scrape Pull & Bear Products

on:
  # Daily at midnight UTC
  schedule:
    - cron: '0 0 * * *'
  # Manual runs
  workflow_dispatch:
    inputs:
      scrape_mode:
        description: 'Scrape mode (full/test)'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - test

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 720  # 12 hours = 720 minutes

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        playwright install chromium
        playwright install-deps chromium

    - name: Run Pull & Bear scraper
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: |
        if [ "${{ github.event.inputs.scrape_mode }}" = "test" ]; then
          echo "Running in test mode with 10 products..."
          python pull_bear_scraper.py --limit 10
        else
          echo "Running full scrape..."
          python pull_bear_scraper.py
        fi

    - name: Upload logs on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs-${{ github.run_id }}
        path: |
          *.log
          logs/
